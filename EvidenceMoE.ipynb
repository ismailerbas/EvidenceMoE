{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd6e24-d9c2-4475-bc75-4d1e2f8b65cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Advancing Fluorescence Light Detection and Ranging in Scattering Media with \n",
    "a Physics-Guided Mixture-of-Experts and Evidential Critics\n",
    "------------------------------------------------------------------------------\n",
    "Version Date: 2025-04-29 \n",
    "\"\"\"\n",
    "\n",
    "# must be the very first torch‚Äêmultiprocessing line in the notebook\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method('fork', force=True)\n",
    "\n",
    "# now all usual imports\n",
    "import os, sys, argparse, math, copy, numpy as np, scipy.io as sio\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "# --- Constants ---\n",
    "EPSILON = 1e-8 # Small value for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00428cd0-0ac0-433e-a841-cacb4df608fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# 1. Configuration \n",
    "######################################\n",
    "class EvidenceMoeConfig:\n",
    "    \"\"\"Configuration class for the EvidenceMoE model with EDC.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=1,\n",
    "        hidden_size=224,\n",
    "        intermediate_size=384,\n",
    "        num_hidden_layers=2,\n",
    "        num_attention_heads=16,\n",
    "        num_experts=3,\n",
    "        max_position_embeddings=512,\n",
    "        initializer_range=0.02,\n",
    "        rms_norm_eps=1e-6,\n",
    "        attention_dropout=0.15194,\n",
    "        # Optimizer Hyperparameters\n",
    "        expert_lr: float = 5e-5,         # LR for Experts \n",
    "        decider_lr: float = 1e-4,        # LR for Decider Head\n",
    "        critic_lr: float = 1e-4,         # LR for EDC Critics\n",
    "        expert_weight_decay: float = 0.0, # Weight decay for Experts\n",
    "        decider_weight_decay: float = 0.0, # Weight decay for Decider Head\n",
    "        critic_weight_decay: float = 1e-4, # Weight decay for EDC Critics\n",
    "        # EDC Loss Hyperparameters\n",
    "        lambda_KL: float = 1e-3,\n",
    "        gamma_penalty: float = 1e-3,\n",
    "        quality_kappa: float = 20.0,      # Kappa for 1 / (1 + kappa * MAE) target quality\n",
    "        # Loss Component Weights\n",
    "        lambda_primary: float = 1.0,       # Weight for primary MAE loss\n",
    "        lambda_aux: float = 1.0,           # Weight for auxiliary expert loss\n",
    "        lambda_critic_quality: float = 1.0, # Weight for combined Evi+KL quality loss\n",
    "        lambda_corr: float = 1.0,           # Weight for correction accuracy loss\n",
    "        lambda_penalty: float = 1.0,        # Weight for evidence penalty loss term\n",
    "        lambda_diversity: float = 0.0,      # Weight for diversity loss\n",
    "        # Other Model Params\n",
    "        damping_factor=0.1,\n",
    "        # RL Params (Inactive)\n",
    "        rl_weight=0.0,\n",
    "        rl_entropy_coeff=0.01,\n",
    "        rl_similarity_penalty=0.5,\n",
    "        # Ablation Flags\n",
    "        ablate_quality_weighting: bool = False,\n",
    "        ablate_correction: bool = False,\n",
    "        ablate_quality_in_gating: bool = False,\n",
    "        ablate_decider_feature: bool = False,\n",
    "        ablate_decider_feature_fusion: bool = False,\n",
    "        ablate_uniform_gating: bool = False,\n",
    "        ablate_gating_dropout: bool = False,\n",
    "        ablate_phased_training: bool = False,\n",
    "        ablate_mean_pooling: bool = False,\n",
    "        ablate_auxiliary_mae: bool = False,\n",
    "    ):\n",
    "        # Assign all parameters to self\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.num_experts = num_experts\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.initializer_range = initializer_range\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.expert_lr = expert_lr        \n",
    "        self.decider_lr = decider_lr     \n",
    "        self.critic_lr = critic_lr\n",
    "        self.expert_weight_decay = expert_weight_decay \n",
    "        self.decider_weight_decay = decider_weight_decay \n",
    "        self.critic_weight_decay = critic_weight_decay\n",
    "        self.lambda_KL = lambda_KL\n",
    "        self.gamma_penalty = gamma_penalty\n",
    "        self.quality_kappa = quality_kappa\n",
    "        self.damping_factor = damping_factor\n",
    "        self.rl_weight = rl_weight\n",
    "        self.rl_entropy_coeff = rl_entropy_coeff\n",
    "        self.rl_similarity_penalty = rl_similarity_penalty\n",
    "        self.global_seq_len = 70\n",
    "        self.early_seq_len = 35\n",
    "        self.late_seq_len = 35\n",
    "        self.conv_kernel = 3\n",
    "        self.num_filters = hidden_size\n",
    "        # Ablations\n",
    "        self.ablate_quality_weighting = ablate_quality_weighting\n",
    "        self.ablate_correction = ablate_correction\n",
    "        self.ablate_quality_in_gating = ablate_quality_in_gating\n",
    "        self.ablate_decider_feature = ablate_decider_feature\n",
    "        self.ablate_decider_feature_fusion = ablate_decider_feature_fusion\n",
    "        self.ablate_uniform_gating = ablate_uniform_gating\n",
    "        self.ablate_gating_dropout = ablate_gating_dropout\n",
    "        self.ablate_phased_training = ablate_phased_training\n",
    "        self.ablate_mean_pooling = ablate_mean_pooling\n",
    "        self.ablate_auxiliary_mae = ablate_auxiliary_mae\n",
    "        # Loss Weights\n",
    "        self.lambda_primary = lambda_primary \n",
    "        self.lambda_aux = lambda_aux         \n",
    "        self.lambda_critic_quality = lambda_critic_quality\n",
    "        self.lambda_corr = lambda_corr\n",
    "        self.lambda_penalty = lambda_penalty\n",
    "        self.lambda_diversity = lambda_diversity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca14d57-27a8-489d-8166-86cd85c2be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# 2. Common Components\n",
    "######################################\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Standard sinusoidal positional encoding for transformer inputs.\"\"\"\n",
    "    def __init__(self, embed_dim, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float) * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        L = x.size(1); return x + self.pe[:L].unsqueeze(0).to(x.device)\n",
    "\n",
    "class StandardTransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"A single layer of the standard Transformer encoder.\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout, batch_first=True)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "    def forward(self, src):\n",
    "        attn_output, attn_weights = self.self_attn(src, src, src)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src = self.norm1(src)\n",
    "        ff_output = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(ff_output)\n",
    "        src = self.norm2(src)\n",
    "        return src, attn_weights\n",
    "\n",
    "class StandardTransformerEncoder(nn.Module):\n",
    "    \"\"\"Stack of N standard Transformer encoder layers.\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([StandardTransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "    def forward(self, src):\n",
    "        all_attn_weights = []\n",
    "        output = src\n",
    "        for layer in self.layers:\n",
    "            output, attn_weights = layer(output)\n",
    "            all_attn_weights.append(attn_weights)\n",
    "        return output, all_attn_weights\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    \"\"\"Simple attention-pooling layer.\"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Sequential(nn.Linear(input_dim, input_dim), nn.Tanh(), nn.Linear(input_dim, 1))\n",
    "    def forward(self, x):\n",
    "        weights = self.attn(x); weights = torch.softmax(weights, dim=1); pooled = torch.sum(x * weights, dim=1); return pooled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad0805-9a6e-4f01-b08b-5ff1d31ac217",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# 3. Hybrid CNN-Transformer Encoder\n",
    "######################################\n",
    "class HybridCNNTransformerEncoder(nn.Module):\n",
    "    \"\"\"CNN front-end + Transformer encoder.\"\"\"\n",
    "    def __init__(self, config: EvidenceMoeConfig, conv_kernel=None, num_filters=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        conv_kernel = conv_kernel if conv_kernel is not None else config.conv_kernel\n",
    "        num_filters = num_filters if num_filters is not None else config.num_filters\n",
    "        self.residual_conv = nn.Conv1d(in_channels=config.input_size, out_channels=num_filters, kernel_size=1)\n",
    "        self.conv_initial = nn.Conv1d(in_channels=config.input_size, out_channels=num_filters, kernel_size=conv_kernel, padding=conv_kernel//2)\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters, kernel_size=conv_kernel, padding=conv_kernel//2)\n",
    "        self.ln_conv = nn.LayerNorm(num_filters)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim=num_filters, max_len=config.max_position_embeddings)\n",
    "        self.transformer_encoder = StandardTransformerEncoder(d_model=num_filters, nhead=config.num_attention_heads, dim_feedforward=config.intermediate_size, num_layers=config.num_hidden_layers, dropout=config.attention_dropout)\n",
    "        self.norm = nn.LayerNorm(num_filters)\n",
    "    def forward(self, x):\n",
    "        x_unsqueezed = x.unsqueeze(1); residual = self.residual_conv(x_unsqueezed); c = self.conv_initial(x_unsqueezed); c = self.relu(c); c = self.conv1(c)\n",
    "        c_transposed = c.transpose(1, 2); c_normed = self.ln_conv(c_transposed); c = c_normed.transpose(1, 2)\n",
    "        c = self.dropout1(c); c = c + residual; c = self.relu(c); t = c.transpose(1, 2); t = self.pos_encoding(t)\n",
    "        t, attn_maps = self.transformer_encoder(t); t = self.norm(t); t = torch.nan_to_num(t, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "        return t, attn_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd166cd-6475-49ff-b17c-f898254c1914",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# 4. Expert Branch\n",
    "######################################\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"Expert network predicting mean value(s).\"\"\"\n",
    "    def __init__(self, config: EvidenceMoeConfig, input_seq_length, output_dim=2, return_attn=False):\n",
    "        super().__init__(); self.seq_len = input_seq_length; self.config = config; self.output_dim = output_dim; self.return_attn = return_attn\n",
    "        self.hybrid_encoder = HybridCNNTransformerEncoder(config); self.pooling = AttentionPooling(config.hidden_size)\n",
    "        self.aux_head_mean = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size), nn.ReLU(), nn.Linear(config.hidden_size, config.hidden_size // 2), nn.ReLU(), nn.Linear(config.hidden_size // 2, output_dim))\n",
    "    def forward(self, x):\n",
    "        B, L = x.shape; \n",
    "        encoded, attn_maps = self.hybrid_encoder(x)\n",
    "        features = encoded.mean(dim=1) if self.config.ablate_mean_pooling else self.pooling(encoded)\n",
    "        mean = self.aux_head_mean(features); log_prob = torch.zeros(B, 1, device=mean.device); entropy = torch.zeros(B, 1, device=mean.device) \n",
    "        if self.return_attn: return features, mean, log_prob, entropy, mean, attn_maps\n",
    "        else: return features, mean, log_prob, entropy, mean, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a34a879-44b1-473f-aa99-446e8815f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# 5. Evidence-based Dirichlet Critic\n",
    "######################################\n",
    "class EvidenceCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Evidence-based Dirichlet Critic (EDC) that outputs alpha, beta for quality\n",
    "    (Beta distribution) and a correction signal. Uses shared backbone. Input includes features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim): # input_dim includes features + aux_pred dim\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # Shared Backbone Network\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32), nn.LayerNorm(32), nn.GELU(), nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16), nn.LayerNorm(16), nn.GELU(), nn.Dropout(0.2),\n",
    "        )\n",
    "        # Define slicing indices based on output_dim for evidence head\n",
    "        self.evi_pos_indices = slice(0, output_dim)\n",
    "        self.evi_neg_indices = slice(output_dim, 2 * output_dim)\n",
    "\n",
    "        # Head 1: Evidence Prediction -> outputs raw evidence (pre-softplus)\n",
    "        self.evi_head = nn.Linear(16, output_dim * 2)\n",
    "\n",
    "        # Head 2: Correction Signal Prediction\n",
    "        self.corr_head = nn.Linear(16, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Concatenated [expert_features, expert_aux_pred], shape (B, input_dim)\n",
    "        h = self.backbone(x) # Shared features, shape (B, 16)\n",
    "        # --- Quality Branch (Evidence -> Alpha, Beta) ---\n",
    "        raw_evidence = self.evi_head(h) # Shape (B, output_dim * 2)\n",
    "        evidence_pos = F.softplus(raw_evidence[:, self.evi_pos_indices]) # Shape (B, output_dim)\n",
    "        evidence_neg = F.softplus(raw_evidence[:, self.evi_neg_indices]) # Shape (B, output_dim)\n",
    "        alpha = evidence_pos + 1.0 # Shape (B, output_dim)\n",
    "        beta = evidence_neg + 1.0  # Shape (B, output_dim)\n",
    "        # --- Correction Branch ---\n",
    "        correction = self.corr_head(h) # Shape (B, output_dim)\n",
    "        # Return alpha, beta (for quality), and the correction signal\n",
    "        return alpha, beta, correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06c5f4-0c1b-4eae-b099-c014b4c967c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# 6. Final Decider Head\n",
    "######################################\n",
    "class FinalDeciderHead(nn.Module):\n",
    "    \"\"\"Fuses expert outputs using gating\"\"\"\n",
    "   \n",
    "    def __init__(self, decider_input_dim, num_experts, num_quality_inputs, expert_total_dim,\n",
    "                 ablate_decider_feature: bool = False, ablate_decider_feature_fusion: bool = False,\n",
    "                 ablate_uniform_gating: bool = False, ablate_gating_dropout: bool = False):\n",
    "        super().__init__()\n",
    "       \n",
    "        gating_input_dim = expert_total_dim + decider_input_dim + num_quality_inputs\n",
    "        self.gate_layer = nn.Sequential(nn.Linear(gating_input_dim, decider_input_dim), nn.ReLU(), nn.Linear(decider_input_dim, num_experts), nn.Sigmoid())\n",
    "        fusion_input_dim = expert_total_dim + decider_input_dim; self.fusion_layer = nn.Linear(fusion_input_dim, 2)\n",
    "        self.alpha_depth = nn.Parameter(torch.tensor(0.15)); self.beta_depth = nn.Parameter(torch.tensor(0.85))\n",
    "        self.alpha_lifetime = nn.Parameter(torch.tensor(0.65)); self.beta_lifetime = nn.Parameter(torch.tensor(0.35))\n",
    "        self.ablate_decider_feature=ablate_decider_feature; self.ablate_decider_feature_fusion=ablate_decider_feature_fusion; self.ablate_uniform_gating=ablate_uniform_gating; self.ablate_gating_dropout=ablate_gating_dropout\n",
    "        self.num_experts=num_experts; self.num_quality_inputs = num_quality_inputs; # Store num_quality_inputs\n",
    "        self.register_buffer(\"last_gate_weights\", torch.zeros(num_experts))\n",
    "\n",
    "   \n",
    "    def forward(self, corrected_aux, decider_feature, quality_scores):\n",
    "        corrected_concat = torch.cat(corrected_aux, dim=1)\n",
    "        if self.ablate_uniform_gating: gate_weights = torch.full((corrected_concat.size(0), self.num_experts), 1.0/self.num_experts, device=corrected_concat.device)\n",
    "        else:\n",
    "            gating_decider = torch.zeros_like(decider_feature) if self.ablate_decider_feature else decider_feature\n",
    "            #Check against num_quality_inputs\n",
    "            if quality_scores.shape[1] != self.num_quality_inputs: raise ValueError(f\"FinalDeciderHead expected {self.num_quality_inputs} quality scores, got {quality_scores.shape[1]}\")\n",
    "            gating_input = torch.cat([corrected_concat, gating_decider, quality_scores], dim=1); gate_weights = self.gate_layer(gating_input)\n",
    "            if self.training and not self.ablate_gating_dropout:\n",
    "                mask = (torch.rand(gate_weights.size(0), device=gate_weights.device) < 0.2).float().unsqueeze(1)\n",
    "                # Corrected dropout application\n",
    "                gate_weights = gate_weights.clone() # Ensure out-of-place if needed\n",
    "                gate_weights[:, 2:3] = gate_weights[:, 2:3] * (1.0 - mask)\n",
    "        self.last_gate_weights = gate_weights.detach().clone(); early, late, global_expert = corrected_aux[0], corrected_aux[1], corrected_aux[2]\n",
    "        # Apply gating weights\n",
    "        gated_early=early*gate_weights[:,0:1]; gated_late=late*gate_weights[:,1:2]; gated_global=global_expert*gate_weights[:,2:3]\n",
    "        fused_experts = torch.cat([gated_early, gated_late, gated_global], dim=1)\n",
    "        fusion_decider = torch.zeros_like(decider_feature) if self.ablate_decider_feature_fusion else decider_feature\n",
    "        fusion_input = torch.cat([fused_experts, fusion_decider], dim=1); raw_output = self.fusion_layer(fusion_input)\n",
    "        # Apply tanh  \n",
    "        pred_depth = 2.0*(self.alpha_depth+self.beta_depth*torch.tanh(raw_output[:,0:1])); pred_lifetime = 2.0*(self.alpha_lifetime+self.beta_lifetime*torch.tanh(raw_output[:,1:2]))\n",
    "        pred_depth = torch.clamp(pred_depth, 0.0, 2.0); pred_lifetime = torch.clamp(pred_lifetime, 0.0, 2.0); final_pred = torch.cat([pred_depth, pred_lifetime], dim=1)\n",
    "        return final_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6533ebf-4626-479b-8b8f-1effabf18474",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# 8. EvidenceMoe Model \n",
    "######################################\n",
    "class EvidenceMoeModel(nn.Module):\n",
    "    \"\"\" MoE Model using EvidenceCritic \"\"\"\n",
    "    def __init__(self, config: EvidenceMoeConfig, mode=\"full\"):\n",
    "        super().__init__()\n",
    "        self.mode = mode; self.config = config\n",
    "        self.early_seq_len = config.early_seq_len; self.late_seq_len = config.late_seq_len; self.global_seq_len = config.global_seq_len\n",
    "        self.early_expert = Expert(config, input_seq_length=self.early_seq_len, output_dim=1, return_attn=False) # Ensure return_attn is False if not needed\n",
    "        self.late_expert = Expert(config, input_seq_length=self.late_seq_len, output_dim=1, return_attn=False)\n",
    "        self.global_expert = Expert(config, input_seq_length=self.global_seq_len, output_dim=2, return_attn=True) # Global expert returns features needed for critic input\n",
    "        \n",
    "        # Use EvidenceCritic with combined feature+aux input dimension\n",
    "        feat_dim = config.hidden_size\n",
    "        self.critic_early = EvidenceCritic(input_dim=feat_dim+1, output_dim=1) # Hidden Feature (H) + Aux Pred (1)\n",
    "        self.critic_late = EvidenceCritic(input_dim=feat_dim+1, output_dim=1)  # Hidden Feature (H) + Aux Pred (1)\n",
    "        self.critic_global = EvidenceCritic(input_dim=feat_dim+2, output_dim=2) # Hidden Feature (H) + Aux Pred (2)\n",
    "\n",
    "        self.final_head = FinalDeciderHead(\n",
    "            decider_input_dim=config.hidden_size,\n",
    "            num_experts=3,\n",
    "            num_quality_inputs=4, #  Expecting 4 quality scores (e, l, g_d, g_l)\n",
    "            expert_total_dim=4,   #  based on dimensions of corrected_aux\n",
    "            ablate_decider_feature=config.ablate_decider_feature,\n",
    "            ablate_decider_feature_fusion=config.ablate_decider_feature_fusion,\n",
    "            ablate_uniform_gating=config.ablate_uniform_gating,\n",
    "            ablate_gating_dropout=config.ablate_gating_dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == \"pretrain_experts\":\n",
    "            # Pretrain logic remains unchanged\n",
    "            early_in=x[:,:self.early_seq_len]; late_in=x[:,self.early_seq_len:self.early_seq_len+self.late_seq_len]; global_in=x\n",
    "            _, early_sample, _, _, _, _ = self.early_expert(early_in); _, late_sample, _, _, _, _ = self.late_expert(late_in)\n",
    "            features_global, global_sample, _, _, _, global_attn_maps = self.global_expert(global_in)\n",
    "            pred_depth = (early_sample + global_sample[:,0:1])/2.0; pred_lifetime = (late_sample + global_sample[:,1:2])/2.0\n",
    "            final_pred = torch.cat([pred_depth, pred_lifetime], dim=1)\n",
    "            return final_pred, [early_sample, late_sample, global_sample], global_attn_maps # Return maps needed later\n",
    "\n",
    "        else: # Full mode with EvidenceCritic\n",
    "            B, L = x.shape; assert L == self.global_seq_len, f\"Expected length {self.global_seq_len}, got {L}\"\n",
    "            early_in=x[:,:self.early_seq_len]; late_in=x[:,self.early_seq_len:self.early_seq_len+self.late_seq_len]; global_in=x\n",
    "\n",
    "            # --- Get Expert Outputs (Features and Aux Preds) ---\n",
    "            # Need features_early/late even if attn maps aren't returned\n",
    "            features_early, early_sample,_,_,_,_ = self.early_expert(early_in)\n",
    "            features_late, late_sample,_,_,_,_ = self.late_expert(late_in)\n",
    "            features_global, global_sample,_,_,_,global_attn_maps = self.global_expert(global_in)\n",
    "            aux_preds = [early_sample, late_sample, global_sample] # List [(B,1), (B,1), (B,2)]\n",
    "            expert_features = [features_early, features_late, features_global] # List [(B,H), (B,H), (B,H)]\n",
    "\n",
    "            # --- Get Critic Outputs (using detached features + aux_preds as input) ---\n",
    "            # Detach inputs for critic loss calculation path \n",
    "            critic_input_e = torch.cat([expert_features[0].detach(), aux_preds[0].detach()], dim=1)\n",
    "            critic_input_l = torch.cat([expert_features[1].detach(), aux_preds[1].detach()], dim=1)\n",
    "            critic_input_g = torch.cat([expert_features[2].detach(), aux_preds[2].detach()], dim=1)\n",
    "\n",
    "            alpha_e, beta_e, corr_e = self.critic_early(critic_input_e)\n",
    "            alpha_l, beta_l, corr_l = self.critic_late(critic_input_l)\n",
    "            alpha_g, beta_g, corr_g = self.critic_global(critic_input_g)\n",
    "            correction_signals = [corr_e, corr_l, corr_g]\n",
    "\n",
    "            # --- Calculate Mean Quality Scores ---\n",
    "            q_e = alpha_e/(alpha_e+beta_e+EPSILON); q_l = alpha_l/(alpha_l+beta_l+EPSILON)\n",
    "            q_g_vec = alpha_g/(alpha_g+beta_g+EPSILON) # Shape (B, 2)\n",
    "            # Create the (B, 4) quality score vector for gating/weighting \n",
    "            quality_scores_full = torch.cat([q_e, q_l, q_g_vec], dim=1) # Shape (B, 4)\n",
    "\n",
    "            # --- Apply Ablations & Corrections ---\n",
    "            q_g_scalar_for_ablation = q_g_vec.mean(dim=1, keepdim=True)\n",
    "            quality_scores_mean_for_ablation = torch.cat([q_e, q_l, q_g_scalar_for_ablation], dim=1) # (B, 3)\n",
    "            quality_scores_eff_3 = torch.ones_like(quality_scores_mean_for_ablation) if self.config.ablate_quality_weighting else quality_scores_mean_for_ablation\n",
    "\n",
    "            correction_signals_eff = [torch.zeros_like(c).detach() for c in correction_signals] if self.config.ablate_correction else correction_signals\n",
    "            damped_corrections = [corr * self.config.damping_factor for corr in correction_signals_eff]\n",
    "\n",
    "            # ---Ablation: Apply Correction ONLY---\n",
    "            corrected_aux = []\n",
    "            for i, aux in enumerate(aux_preds):\n",
    "                # Apply correction directly\n",
    "                corrected = aux + damped_corrections[i]\n",
    "                corrected_aux.append(corrected)\n",
    "\n",
    "            # --- Prepare Inputs for Final Head ---\n",
    "            decider_feature = features_global # Global expert's pooled features\n",
    "            \n",
    "            gating_quality_full = torch.ones_like(quality_scores_full) if self.config.ablate_quality_in_gating else quality_scores_full # Ablate all 4 if flag is set\n",
    "\n",
    "            # --- Run Final Decider Head ---\n",
    "            final_pred = self.final_head(corrected_aux, decider_feature, gating_quality_full) # Pass B,4 quality vector\n",
    "\n",
    "            # --- Pack Outputs for Loss Calculation ---\n",
    "            # Need original aux_preds, and critic outputs (alpha, beta, corr)\n",
    "        \n",
    "            q_g_scalar = quality_scores_full[:,2:3].mean(dim=1, keepdim=True)\n",
    "            quality_scores_mean = torch.cat([\n",
    "                quality_scores_full[:,0:1],  # q_e\n",
    "                quality_scores_full[:,1:2],  # q_l\n",
    "                q_g_scalar                  # mean of q_g_depth & q_g_lifetime\n",
    "            ], dim=1)\n",
    "            critic_outputs_dict = {\n",
    "                \"alpha_early\": alpha_e, \"beta_early\": beta_e, \"corr_early\": corr_e,\n",
    "                \"alpha_late\":  alpha_l, \"beta_late\":  beta_l, \"corr_late\":  corr_l,\n",
    "                \"alpha_global\": alpha_g, \"beta_global\": beta_g, \"corr_global\": corr_g,\n",
    "                \"quality_scores_full\": quality_scores_full,\n",
    "                \"quality_scores_mean\": quality_scores_mean, \n",
    "            }\n",
    "            \n",
    "\n",
    "            return (final_pred, aux_preds, critic_outputs_dict, expert_features, # Return features needed for critic input in loss calc\n",
    "                    decider_feature, global_attn_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8b9fb-6a48-4a5b-ac99-23be7be299c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# 9. Diversity Loss\n",
    "######################################\n",
    "def diversity_loss(aux_preds, margin=0.3):\n",
    "    loss=0.0; num_pairs=0\n",
    "    for i in range(len(aux_preds)):\n",
    "        for j in range(i+1,len(aux_preds)):\n",
    "            f_i=F.normalize(aux_preds[i],dim=1); f_j=F.normalize(aux_preds[j],dim=1)\n",
    "            cos_sim=(f_i*f_j).sum(dim=1); loss+=torch.mean(torch.clamp(cos_sim-margin,min=0.0)); num_pairs+=1\n",
    "    if num_pairs>0: loss=loss/num_pairs\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b148fe29-bdcc-4fe5-b53f-ef4f6b4b4e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# 10. Aux Error Quality Target Calculation\n",
    "######################################\n",
    "def compute_aux_error_quality_targets(aux_preds, ground_truth, quality_kappa):\n",
    "    \"\"\" Calculates target quality = 1 / (1 + kappa * MAE) based on AUXILIARY errors. \"\"\"\n",
    "    gt_depth = ground_truth[:, 0:1]; gt_lifetime = ground_truth[:, 1:2]\n",
    "    aux_early = aux_preds[0].view_as(gt_depth) if aux_preds[0].shape != gt_depth.shape else aux_preds[0]\n",
    "    aux_late = aux_preds[1].view_as(gt_lifetime) if aux_preds[1].shape != gt_lifetime.shape else aux_preds[1]\n",
    "    aux_global = aux_preds[2].view_as(ground_truth) if aux_preds[2].shape != ground_truth.shape else aux_preds[2]\n",
    "    mae_early = torch.abs(aux_early - gt_depth)\n",
    "    mae_late = torch.abs(aux_late - gt_lifetime)\n",
    "    mae_global = torch.abs(aux_global - ground_truth)\n",
    "    quality_target_early = 1.0 / (1.0 + quality_kappa * (mae_early + EPSILON))\n",
    "    quality_target_late = 1.0 / (1.0 + quality_kappa * (mae_late + EPSILON))\n",
    "    quality_target_global_vec = 1.0 / (1.0 + quality_kappa * (mae_global + EPSILON))\n",
    "    return quality_target_early, quality_target_late, quality_target_global_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd37433-1e04-4056-ad56-8147dc93a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# 12. Training Function\n",
    "######################################\n",
    "\n",
    "# --- Define EDC Loss Helper Functions ---\n",
    "def compute_evidential_loss(alpha, beta, target_q_gt):\n",
    "    \"\"\" Calculates L_evi = MSE(pred_mean, target) + Variance(pred). \"\"\"\n",
    "    alpha_stable = alpha + EPSILON; beta_stable = beta + EPSILON; S = alpha_stable + beta_stable\n",
    "    q_pred = alpha_stable / S\n",
    "    mse_term = (target_q_gt - q_pred)**2\n",
    "    variance_term = (alpha_stable * beta_stable) / (S.pow(2) * (S + 1.0))\n",
    "    L_evi = mse_term + variance_term\n",
    "    return L_evi # Shape: (B, out_dim)\n",
    "\n",
    "def compute_kl_divergence_loss(alpha, beta):\n",
    "    \"\"\" Calculates KL( Beta(alpha, beta) || Beta(1, 1) ). \"\"\"\n",
    "    alpha_stable = alpha + EPSILON; beta_stable = beta + EPSILON\n",
    "    log_beta_posterior = torch.lgamma(alpha_stable) + torch.lgamma(beta_stable) - torch.lgamma(alpha_stable + beta_stable)\n",
    "    term1 = -log_beta_posterior\n",
    "    term2 = (alpha_stable - 1.0) * (torch.digamma(alpha_stable) - torch.digamma(alpha_stable + beta_stable))\n",
    "    term3 = (beta_stable - 1.0) * (torch.digamma(beta_stable) - torch.digamma(alpha_stable + beta_stable))\n",
    "    L_KL = torch.clamp(term1 + term2 + term3, min=0)\n",
    "    return L_KL # Shape: (B, out_dim)\n",
    "\n",
    "def compute_evidence_penalty_loss(alpha, beta, correction, gamma):\n",
    "    \"\"\" Calculates L_e-penalty = gamma * correction^2 / S (element-wise). \"\"\"\n",
    "    alpha_stable = alpha + EPSILON; beta_stable = beta + EPSILON; S = alpha_stable + beta_stable\n",
    "    delta_sq = correction**2\n",
    "    if delta_sq.shape[-1] != S.shape[-1]: # Ensure broadcasting works if needed\n",
    "        S_eff = S.mean(dim=-1, keepdim=True) if delta_sq.shape[-1] == 1 else S.expand_as(delta_sq)\n",
    "    else: S_eff = S\n",
    "    L_penalty = gamma * delta_sq / (S_eff + EPSILON) # Add epsilon to denominator\n",
    "    return L_penalty\n",
    "# --- End Loss Helpers ---\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    num_epochs: int = 1000,\n",
    "    pretrain_epochs: int = 20,\n",
    "    integration_epochs: int = 30,\n",
    ") -> float:\n",
    "    \"\"\" Trains the EvidenceMoeModel with EvidenceCritic and 3 Optimizers. \"\"\"\n",
    "    # ‚îÄ‚îÄ‚îÄ unwrap DDP if needed ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    if hasattr(model, \"module\"):\n",
    "        # model was wrapped in DistributedDataParallel\n",
    "        model = model.module\n",
    "    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "    config_suffix = get_config_suffix(model.config)\n",
    "    log_dir = f\"runs/EDC_Critic_{config_suffix}_3opt_k8\" \n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "\n",
    "    # Parameter Groups\n",
    "    experts_params = (list(model.early_expert.parameters()) + list(model.late_expert.parameters()) + list(model.global_expert.parameters()))\n",
    "    decider_params = list(model.final_head.parameters())\n",
    "    critic_params = (list(model.critic_early.parameters()) + list(model.critic_late.parameters()) + list(model.critic_global.parameters()))\n",
    "\n",
    "    # --- Optimizers \n",
    "    expert_lr_base = model.config.expert_lr # Get base LR from config if defined, else use default\n",
    "    optimizer_experts = AdamW(experts_params, lr=expert_lr_base, weight_decay=model.config.expert_weight_decay)\n",
    "    optimizer_decider = AdamW(decider_params, lr=model.config.decider_lr, weight_decay=model.config.decider_weight_decay)\n",
    "    optimizer_critics = AdamW(critic_params, lr=model.config.critic_lr, weight_decay=model.config.critic_weight_decay)\n",
    "    pretrain_optimizer = optimizer_experts # Use expert optimizer during pretrain\n",
    "\n",
    "    # Prepare expert group dict for potential phased LR adjustment\n",
    "    expert_group_config = {\"params\": experts_params, \"lr\": expert_lr_base} # Store config for later adjustment if needed\n",
    "\n",
    "    # Loss Functions\n",
    "    mae_loss_fn = nn.L1Loss()\n",
    "    huber_loss_fn = nn.SmoothL1Loss(reduction='none')\n",
    "\n",
    "    # Grad Scaler\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "    # Training State\n",
    "    best_val_loss = float('inf'); best_model_state = None; patience_counter = 0; patience = 25; global_step = 0\n",
    "\n",
    "    # Main Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Phase Handling ---\n",
    "        if hasattr(train_loader, \"sampler\") and hasattr(train_loader.sampler, \"set_epoch\"):\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "        gradual_unfreeze_epochs = 10\n",
    "        active_optimizers = [] # List of optimizers to step this epoch\n",
    "        current_rl_weight = 0.0\n",
    "\n",
    "        if model.config.ablate_phased_training:\n",
    "            phase = \"FULL JOINT\"\n",
    "            model.mode = \"full\"\n",
    "            for p in model.parameters(): p.requires_grad_(True)\n",
    "            # Set expert LR \n",
    "            for group in optimizer_experts.param_groups: group['lr'] = expert_lr_base\n",
    "            active_optimizers = [optimizer_experts, optimizer_decider, optimizer_critics]\n",
    "            current_rl_weight = model.config.rl_weight\n",
    "            print(f\"\\n--- Epoch {epoch+1}/{num_epochs} :: Phase: {phase} (Ablated) ---\")\n",
    "        else:\n",
    "            if epoch < pretrain_epochs:\n",
    "                phase = \"PRETRAIN\"\n",
    "                model.mode = \"pretrain_experts\"\n",
    "                for p in model.parameters(): p.requires_grad_(False) # Freeze all first\n",
    "                for p in experts_params: p.requires_grad_(True)     # Unfreeze only experts\n",
    "                active_optimizers = [optimizer_experts]            # Only step expert optimizer\n",
    "                current_rl_weight = 0.0\n",
    "                print(f\"\\n--- Epoch {epoch+1}/{num_epochs} :: Phase: {phase} ---\")\n",
    "            elif epoch < integration_epochs:\n",
    "                phase = \"INTEGRATE\"\n",
    "                model.mode = \"full\"\n",
    "                for p in model.parameters(): p.requires_grad_(False) # Freeze all first\n",
    "                for p in decider_params + critic_params: p.requires_grad_(True) # Unfreeze decider & critics\n",
    "                active_optimizers = [optimizer_decider, optimizer_critics] # Step decider & critic optimizers\n",
    "                current_rl_weight = model.config.rl_weight\n",
    "                print(f\"\\n--- Epoch {epoch+1}/{num_epochs} :: Phase: {phase} ---\")\n",
    "            else: # Full Joint Training phase starts\n",
    "                phase = \"JOINT\"\n",
    "                model.mode = \"full\"\n",
    "                for p in model.parameters(): p.requires_grad_(True) # Ensure all are trainable\n",
    "\n",
    "                # Gradual Unfreezing Logic for Experts\n",
    "                if epoch < integration_epochs + gradual_unfreeze_epochs:\n",
    "                    factor = (epoch - integration_epochs + 1) / gradual_unfreeze_epochs\n",
    "                    # Adjust LR only for the expert optimizer group\n",
    "                    for group in optimizer_experts.param_groups: group['lr'] = expert_lr_base * factor\n",
    "                    phase += f\" (Unfrz {factor:.1f})\"\n",
    "                else:\n",
    "                    for group in optimizer_experts.param_groups: group['lr'] = expert_lr_base\n",
    "\n",
    "                active_optimizers = [optimizer_experts, optimizer_decider, optimizer_critics] # Step all three\n",
    "                current_rl_weight = model.config.rl_weight\n",
    "                print(f\"\\n--- Epoch {epoch+1}/{num_epochs} :: Phase: {phase} ---\")\n",
    "        # --- End Phase Handling ---\n",
    "\n",
    "        # --- Training Epoch ---\n",
    "        model.train(); train_loss_sum=0.0; total_samples=0; total_grad_norm=0.0; grad_steps=0\n",
    "        epoch_losses = {\"primary\": 0.0, \"aux\": 0.0, \"quality\": 0.0, \"corr\": 0.0, \"penalty\": 0.0, \"total\": 0.0, \"div\": 0.0}\n",
    "\n",
    "        for batch_idx, (batch_x, batch_y) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} [{phase}]\")):\n",
    "            batch_x = batch_x.to(device); batch_y = batch_y.to(device)\n",
    "\n",
    "            # Zero gradients for active optimizers\n",
    "            for opt in active_optimizers: opt.zero_grad()\n",
    "            \n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=device.type == 'cuda'):\n",
    "                # --- Forward pass ---\n",
    "                outputs = model(batch_x)\n",
    "\n",
    "                # --- Loss Calculation ---\n",
    "                if model.mode == \"pretrain_experts\":\n",
    "                    final_pred, expert_outputs, _ = outputs\n",
    "                    loss_e = mae_loss_fn(expert_outputs[0].squeeze(-1), batch_y[:, 0]); loss_l = mae_loss_fn(expert_outputs[1].squeeze(-1), batch_y[:, 1]); loss_g = mae_loss_fn(expert_outputs[2], batch_y)\n",
    "                    total_loss = (3.0 * loss_e + loss_l + loss_g) / 5.0 # Weighted pretrain loss\n",
    "                    epoch_losses[\"primary\"] += total_loss.item() * batch_x.size(0); epoch_losses[\"total\"] += total_loss.item() * batch_x.size(0)\n",
    "                    # Placeholders for other losses\n",
    "                    primary_loss=torch.tensor(0.0); aux_loss=torch.tensor(0.0); critic_quality_loss=torch.tensor(0.0); correction_loss=torch.tensor(0.0); evidence_penalty_loss=torch.tensor(0.0); rl_loss=torch.tensor(0.0); div_loss=torch.tensor(0.0)\n",
    "\n",
    "                else: # Full mode or Integration mode\n",
    "                    (final_pred, aux_preds, critic_outputs, expert_features, # Get features for detached critic input\n",
    "                     rl_log_probs, rl_entropies, decider_feature, _) = outputs\n",
    "\n",
    "                    # 1. Primary Loss (Always calculated)\n",
    "                    primary_loss = mae_loss_fn(final_pred, batch_y)\n",
    "\n",
    "                    # 2. Auxiliary Loss \n",
    "                    if model.config.ablate_auxiliary_mae: aux_loss = torch.tensor(0.0, device=device)\n",
    "                    else: loss_e_aux = F.l1_loss(aux_preds[0], batch_y[:,0:1]); loss_l_aux = F.l1_loss(aux_preds[1], batch_y[:,1:2]); loss_g_aux = F.l1_loss(aux_preds[2], batch_y); aux_loss=(loss_e_aux+loss_l_aux+loss_g_aux.mean())/3.0\n",
    "\n",
    "                    # --- Critic Loss Calculations (Only if critics are being trained) ---\n",
    "                    if phase != \"PRETRAIN\":\n",
    "                        # 3. Calculate Aux Error Quality Targets\n",
    "                        with torch.no_grad(): target_q_e, target_q_l, target_q_g_vec = compute_aux_error_quality_targets(aux_preds, batch_y, quality_kappa=model.config.quality_kappa); target_q_g_d=target_q_g_vec[:,0:1]; target_q_g_l=target_q_g_vec[:,1:2]\n",
    "\n",
    "                        # --- Get Critic Outputs using inputs for loss calc ---\n",
    "                        critic_input_e = torch.cat([expert_features[0].detach(), aux_preds[0].detach()], dim=1)\n",
    "                        critic_input_l = torch.cat([expert_features[1].detach(), aux_preds[1].detach()], dim=1)\n",
    "                        critic_input_g = torch.cat([expert_features[2].detach(), aux_preds[2].detach()], dim=1)\n",
    "                        alpha_e, beta_e, corr_e = model.critic_early(critic_input_e)\n",
    "                        alpha_l, beta_l, corr_l = model.critic_late(critic_input_l)\n",
    "                        alpha_g, beta_g, corr_g = model.critic_global(critic_input_g)\n",
    "                        # We use alpha/beta/corr for loss calculation below\n",
    "\n",
    "                        # 4. Critic Quality Loss (Evidential + KL)\n",
    "                        L_evi_e=compute_evidential_loss(alpha_e,beta_e,target_q_e); L_KL_e=compute_kl_divergence_loss(alpha_e,beta_e)\n",
    "                        L_evi_l=compute_evidential_loss(alpha_l,beta_l,target_q_l); L_KL_l=compute_kl_divergence_loss(alpha_l,beta_l)\n",
    "                        alpha_g_d,beta_g_d=alpha_g[:,0:1],beta_g[:,0:1]; alpha_g_l,beta_g_l=alpha_g[:,1:2],beta_g[:,1:2]\n",
    "                        L_evi_g_d=compute_evidential_loss(alpha_g_d,beta_g_d,target_q_g_d); L_KL_g_d=compute_kl_divergence_loss(alpha_g_d,beta_g_d)\n",
    "                        L_evi_g_l=compute_evidential_loss(alpha_g_l,beta_g_l,target_q_g_l); L_KL_g_l=compute_kl_divergence_loss(alpha_g_l,beta_g_l)\n",
    "                        critic_quality_loss = torch.mean( L_evi_e+L_evi_l+L_evi_g_d+L_evi_g_l + model.config.lambda_KL*(L_KL_e+L_KL_l+L_KL_g_d+L_KL_g_l) )\n",
    "\n",
    "                        # 5. Correction Loss (Huber) - Use aux_preds and correction signals from main forward pass\n",
    "                        # Get the correction signals generated by the forward pass inside the model\n",
    "                        corr_e_fwd, corr_l_fwd, corr_g_fwd = critic_outputs[\"corr_early\"], critic_outputs[\"corr_late\"], critic_outputs[\"corr_global\"]\n",
    "                        if not model.config.ablate_correction:\n",
    "                            corr_loss_e=huber_loss_fn(aux_preds[0]+model.config.damping_factor*corr_e_fwd, batch_y[:,0:1]) \n",
    "                            corr_loss_l=huber_loss_fn(aux_preds[1]+model.config.damping_factor*corr_l_fwd, batch_y[:,1:2])\n",
    "                            corr_loss_g=huber_loss_fn(aux_preds[2]+model.config.damping_factor*corr_g_fwd, batch_y)\n",
    "                            correction_loss = (torch.mean(corr_loss_e)+torch.mean(corr_loss_l)+torch.mean(corr_loss_g))/3.0\n",
    "                        else: correction_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "                        # 6. Evidence Penalty Loss \n",
    "                        if not model.config.ablate_correction:\n",
    "                            L_pen_e=compute_evidence_penalty_loss(alpha_e,beta_e,corr_e,model.config.gamma_penalty) \n",
    "                            L_pen_l=compute_evidence_penalty_loss(alpha_l,beta_l,corr_l,model.config.gamma_penalty)\n",
    "                            L_pen_g_d=compute_evidence_penalty_loss(alpha_g_d,beta_g_d,corr_g[:,0:1],model.config.gamma_penalty)\n",
    "                            L_pen_g_l=compute_evidence_penalty_loss(alpha_g_l,beta_g_l,corr_g[:,1:2],model.config.gamma_penalty)\n",
    "                            evidence_penalty_loss = torch.mean(L_pen_e + L_pen_l + L_pen_g_d + L_pen_g_l)\n",
    "                        else: evidence_penalty_loss = torch.tensor(0.0, device=device)\n",
    "                    else: # If PRETRAIN, critic losses are zero\n",
    "                        critic_quality_loss = torch.tensor(0.0, device=device)\n",
    "                        correction_loss = torch.tensor(0.0, device=device)\n",
    "                        evidence_penalty_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "\n",
    "                    # --- Optional Losses ---\n",
    "                    div_loss = diversity_loss(aux_preds, margin=0.3) * model.config.lambda_diversity # Apply weight here\n",
    "            \n",
    "\n",
    "                    # --- Total Loss Combination ---\n",
    "                    # Note: We calculate ALL losses, but only gradients relevant to the active optimizers will be used.\n",
    "                    total_loss = ( model.config.lambda_primary * primary_loss\n",
    "                                  + model.config.lambda_aux * aux_loss * (0.0 if model.config.ablate_auxiliary_mae else 1.0)\n",
    "                                  + model.config.lambda_critic_quality * critic_quality_loss\n",
    "                                  + model.config.lambda_corr * correction_loss\n",
    "                                  + model.config.lambda_penalty * evidence_penalty_loss\n",
    "                                  + model.config.lambda_diversity * div_loss\n",
    "                                  + model.config.rl_weight * rl_loss\n",
    "                                 )\n",
    "\n",
    "                    # Store epoch losses\n",
    "                    epoch_losses[\"primary\"] += primary_loss.item() * batch_x.size(0)\n",
    "                    epoch_losses[\"aux\"] += aux_loss.item() * batch_x.size(0)\n",
    "                    epoch_losses[\"quality\"] += critic_quality_loss.item() * batch_x.size(0)\n",
    "                    epoch_losses[\"corr\"] += correction_loss.item() * batch_x.size(0)\n",
    "                    epoch_losses[\"penalty\"] += evidence_penalty_loss.item() * batch_x.size(0)\n",
    "                    epoch_losses[\"rl\"] += rl_loss.item() * batch_x.size(0)\n",
    "                    epoch_losses[\"div\"] += div_loss.item() * batch_x.size(0)\n",
    "                    epoch_losses[\"total\"] += total_loss.item() * batch_x.size(0)\n",
    "\n",
    "            # --- Backprop & Step (Using Active Optimizers) ---\n",
    "            loss_val = total_loss.item()\n",
    "            if not np.isfinite(loss_val): print(f\"[ERROR] Loss is NaN/Inf at E{epoch+1} B{batch_idx+1}. Skipping.\"); [opt.zero_grad() for opt in active_optimizers]; continue\n",
    "            if loss_val > 1e5: print(f\"[WARN] High loss: {loss_val:.4f} at E{epoch+1} B{batch_idx+1}\")\n",
    "\n",
    "            scaler.scale(total_loss).backward() # Calculate gradients for the whole graph\n",
    "\n",
    "            # Unscale and step ONLY the active optimizers for the current phase\n",
    "            for opt in active_optimizers: scaler.unscale_(opt)\n",
    "\n",
    "            # Clip gradients for the whole model after unscaling active optimizers\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Step active optimizers\n",
    "            for opt in active_optimizers: scaler.step(opt)\n",
    "\n",
    "            scaler.update() # Update scaler once\n",
    "            \n",
    "\n",
    "            # --- Accumulate Batch Info ---\n",
    "            global_step += 1; bs = batch_x.size(0); total_samples += bs\n",
    "           \n",
    "            total_grad_norm += grad_norm.item() if torch.is_tensor(grad_norm) else grad_norm; grad_steps += 1\n",
    "\n",
    "            # --- Log Batch Details Periodically  ---\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                 log_prefix = f\"E{epoch+1} B{batch_idx+1}\"\n",
    "                 if model.mode == \"pretrain_experts\":\n",
    "                     print(f\"{log_prefix}: Pretrain Loss={total_loss.item():.4f}\")\n",
    "                 else:\n",
    "                     # Log detailed losses\n",
    "                     print(f\"{log_prefix}: Loss={total_loss.item():.4f} [Pri={primary_loss.item():.4f} Aux={aux_loss.item():.4f} Q={critic_quality_loss.item():.4f} C={correction_loss.item():.4f} P={evidence_penalty_loss.item():.4f} RL={rl_loss.item():.4f} Div={div_loss.item():.4f}] Grad={grad_norm:.2f}\")\n",
    "                     # Log Quality Stats (Mean Quality)\n",
    "                     with torch.no_grad():\n",
    "                        quality_scores_mean_print = critic_outputs[\"quality_scores_mean\"] # Shape (B, 3)\n",
    "                        q_stats = {\"min\": quality_scores_mean_print.min().item(), \"max\": quality_scores_mean_print.max().item(),\n",
    "                                   \"mean\": quality_scores_mean_print.mean().item(), \"std\": quality_scores_mean_print.std().item()}\n",
    "                        print(f\"    Quality (Mean) Stats: {q_stats}\")\n",
    "                     # Log Correction Stats\n",
    "                     correction_signals = [corr_e, corr_l, corr_g]\n",
    "                     with torch.no_grad():\n",
    "                         # Use correction_signals list \n",
    "                         correction_signals_print = [c.detach().cpu() for c in correction_signals]\n",
    "                         for i, cs in enumerate(correction_signals_print):\n",
    "                             min_v = cs.min().item(); max_v = cs.max().item(); mean_v = cs.mean().item(); std_v = cs.std().item()\n",
    "                             print(f\"    Correction Signal [{i}] Stats: min={min_v:.3f} max={max_v:.3f} mean={mean_v:.3f} std={std_v:.3f}\")\n",
    "                     # Log Decider Feature Stats\n",
    "                     with torch.no_grad():\n",
    "                         decider_stats = {\"min\": decider_feature.min().item(), \"max\": decider_feature.max().item(),\n",
    "                                          \"mean\": decider_feature.mean().item(), \"std\": decider_feature.std().item()}\n",
    "                         print(f\"    Decider Feature Stats: {decider_stats}\")\n",
    "                 sys.stdout.flush()\n",
    "\n",
    "        # --- End of Epoch ---\n",
    "        avg_grad_norm = total_grad_norm / grad_steps if grad_steps > 0 else 0.0\n",
    "        if total_samples > 0:\n",
    "             for key in epoch_losses: epoch_losses[key] /= total_samples\n",
    "        else:\n",
    "             for key in epoch_losses: epoch_losses[key] = 0.0 # Avoid NaN\n",
    "\n",
    "        # Write Epoch Summaries to TensorBoard\n",
    "        writer.add_scalar(\"Loss/Train_Total\", epoch_losses[\"total\"], epoch)\n",
    "        writer.add_scalar(\"Grad_Norm/Avg\", avg_grad_norm, epoch)\n",
    "        # Log LRs for active optimizers\n",
    "        for i, opt in enumerate([optimizer_experts, optimizer_decider, optimizer_critics]):\n",
    "             if opt in active_optimizers: # Log LR if optimizer was used\n",
    "                 writer.add_scalar(f\"Learning_Rate/Group_{i}\", opt.param_groups[0]['lr'], epoch)\n",
    "\n",
    "        if model.mode != \"pretrain_experts\":\n",
    "            writer.add_scalar(\"Loss/Primary\", epoch_losses[\"primary\"], epoch)\n",
    "            writer.add_scalar(\"Loss/Auxiliary\", epoch_losses[\"aux\"], epoch)\n",
    "            writer.add_scalar(\"Loss/Critic_Quality\", epoch_losses[\"quality\"], epoch)\n",
    "            writer.add_scalar(\"Loss/Critic_Correction\", epoch_losses[\"corr\"], epoch)\n",
    "            writer.add_scalar(\"Loss/Critic_Penalty\", epoch_losses[\"penalty\"], epoch)\n",
    "            writer.add_scalar(\"Loss/RL\", epoch_losses[\"rl\"], epoch)\n",
    "            writer.add_scalar(\"Loss/Diversity\", epoch_losses[\"div\"], epoch)\n",
    "            writer.add_histogram(\"GateWeights\", model.final_head.last_gate_weights.cpu(), epoch)\n",
    "            # Add alpha/beta histograms from last batch if critic_outputs exists\n",
    "            if 'critic_outputs' in locals() and critic_outputs:\n",
    "                 writer.add_histogram(\"Quality/Alpha_Early\", critic_outputs[\"alpha_early\"].detach().cpu(), epoch)\n",
    "                 writer.add_histogram(\"Quality/Beta_Early\", critic_outputs[\"beta_early\"].detach().cpu(), epoch)\n",
    "                 writer.add_histogram(\"Correction/Early\", critic_outputs[\"corr_early\"].detach().cpu(), epoch)\n",
    "                 # Add Late and Global if needed\n",
    "\n",
    "        # Print Epoch Summary \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} -> Avg Train Loss={epoch_losses['total']:.6f}, Avg Grad Norm: {avg_grad_norm:.4f}\")\n",
    "        if model.mode != \"pretrain_experts\":\n",
    "            print(f\"  Avg Losses: Pri={epoch_losses['primary']:.4f} Aux={epoch_losses['aux']:.4f} Q={epoch_losses['quality']:.4f} C={epoch_losses['corr']:.4f} P={epoch_losses['penalty']:.4f} RL={epoch_losses['rl']:.4f} Div={epoch_losses['div']:.4f}\")\n",
    "\n",
    "        # --- Validation Epoch ---\n",
    "        model.eval(); val_loss_sum = 0.0; num_val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x=batch_x.to(device); batch_y=batch_y.to(device)\n",
    "                with torch.amp.autocast(device_type=device.type, enabled=device.type == 'cuda'):\n",
    "                     if model.mode == \"pretrain_experts\": final_pred, _, _ = model(batch_x)\n",
    "                     else: final_pred, _, _, _, _, _, _, _ = model(batch_x)\n",
    "                     loss = mae_loss_fn(final_pred, batch_y)\n",
    "                bs=batch_x.size(0); val_loss_sum+=loss.item()*bs; num_val_samples+=bs\n",
    "            epoch_val_loss = val_loss_sum / num_val_samples if num_val_samples > 0 else float('inf')\n",
    "            writer.add_scalar(\"Loss/Validation\", epoch_val_loss, epoch)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} -> VAL_LOSS={epoch_val_loss:.6f}\") # This is Primary MAE Loss\n",
    "\n",
    "            # --- Sample Predictions ---\n",
    "            try:\n",
    "                # Use a separate loader for consistent sampling if needed, or just grab first batch\n",
    "                val_batch_x_sample, val_batch_y_sample = next(iter(val_loader))\n",
    "                val_batch_x_sample=val_batch_x_sample.to(device); val_batch_y_sample=val_batch_y_sample.to(device)\n",
    "                with torch.amp.autocast(device_type=device.type, enabled=device.type=='cuda'):\n",
    "                    model.eval() # Ensure model is in eval mode\n",
    "                    if model.mode == \"pretrain_experts\": final_pred_val, _, _ = model(val_batch_x_sample); critic_outputs_val=None\n",
    "                    else: final_pred_val, _, critic_outputs_val, _, _, _, _, _ = model(val_batch_x_sample)\n",
    "\n",
    "                val_batch_y_cpu=val_batch_y_sample.cpu(); final_pred_cpu=final_pred_val.cpu()\n",
    "                quality_vals_print_map = {} # Use map for index safety\n",
    "                batch_size_val = val_batch_x_sample.size(0)\n",
    "                num_examples = min(5, batch_size_val)\n",
    "                indices = np.random.choice(batch_size_val, num_examples, replace=False)\n",
    "\n",
    "                if critic_outputs_val and \"quality_scores_mean\" in critic_outputs_val:\n",
    "                    q_mean_cpu = critic_outputs_val[\"quality_scores_mean\"].cpu()\n",
    "                    for sample_idx in indices:\n",
    "                        if sample_idx < q_mean_cpu.shape[0]: # Check index bounds\n",
    "                            q_vals = q_mean_cpu[sample_idx].tolist()\n",
    "                            quality_vals_print_map[sample_idx] = f\"[{q_vals[0]:.2f},{q_vals[1]:.2f},{q_vals[2]:.2f}]\"\n",
    "                        else: quality_vals_print_map[sample_idx] = \"N/A (Index out of bounds)\"\n",
    "                else:\n",
    "                    for idx in indices: quality_vals_print_map[idx] = \"N/A\"\n",
    "\n",
    "                print(\"Random validation sample predictions:\")\n",
    "                for idx in indices: # Iterate through selected indices\n",
    "                    gt_d=val_batch_y_cpu[idx,0].item(); pred_d=final_pred_cpu[idx,0].item()\n",
    "                    gt_l=val_batch_y_cpu[idx,1].item(); pred_l=final_pred_cpu[idx,1].item()\n",
    "                    log_str = f\"  Ex {idx}: GT D:{gt_d:.3f} P:{pred_d:.3f} | GT L:{gt_l:.3f} P:{pred_l:.3f}\"\n",
    "                    log_str += f\" | Q(E,L,G_mean):{quality_vals_print_map.get(idx, 'Error')}\"\n",
    "                    print(log_str)\n",
    "            except StopIteration: print(\"Validation loader empty, cannot show sample predictions.\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # --- Checkpoint Saving ---\n",
    "        if model.mode != \"pretrain_experts\":\n",
    "            if epoch_val_loss < best_val_loss:\n",
    "                best_val_loss = epoch_val_loss; best_model_state = model.state_dict()\n",
    "                best_ckpt = f\"best_checkpoint_{config_suffix}.pth\"\n",
    "                if dist.get_rank() == 0:\n",
    "                    torch.save(best_model_state, best_ckpt); print(f\"    Best model saved (Val Loss: {best_val_loss:.6f}) to {best_ckpt}\"); patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience: print(f\"Early stopping triggered after {epoch+1} epochs.\"); break\n",
    "\n",
    "    # --- End of Training ---\n",
    "    final_ckpt = f\"final_{config_suffix}.pth\"\n",
    "    if best_model_state: print(f\"Loading best state (Val Loss: {best_val_loss:.6f})\"); model.load_state_dict(best_model_state)\n",
    "    else: print(\"No best state found or only pretraining done, saving last state.\")\n",
    "        \n",
    "    if dist.get_rank() == 0:\n",
    "        torch.save(model.state_dict(), final_ckpt); print(f\"Final model saved to {final_ckpt}\")\n",
    "        writer.add_text(\"Training Summary\", f\"Complete. Best val loss: {best_val_loss:.6f}. Saved: {final_ckpt}\", 0); writer.close()\n",
    "        print(f\"Training complete. Best val loss: {best_val_loss:.6f}\")\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94969af-0ca4-4c1d-a021-66cca11beda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Helper: Suffix Generation \n",
    "######################################\n",
    "def get_config_suffix(config: EvidenceMoeConfig):\n",
    "    \"\"\"Generates a filename suffix based on active ablations in the config.\"\"\"\n",
    "    parts = []\n",
    "    if config.ablate_quality_weighting: parts.append(\"noQualW\")\n",
    "    if config.ablate_correction:        parts.append(\"noCorr\")\n",
    "    if config.ablate_quality_in_gating: parts.append(\"noQualGate\")\n",
    "    if config.ablate_decider_feature:   parts.append(\"noDecFeat\")\n",
    "    if config.ablate_decider_feature_fusion: parts.append(\"noDecFus\")\n",
    "    if config.ablate_uniform_gating:    parts.append(\"uniGate\")\n",
    "    if config.ablate_gating_dropout:    parts.append(\"noGateDrop\")\n",
    "    if config.ablate_phased_training:   parts.append(\"noPhase\")\n",
    "    if config.ablate_mean_pooling:      parts.append(\"meanPool\")\n",
    "    if config.ablate_auxiliary_mae:     parts.append(\"noAuxMAE\")\n",
    "    return \"_k2_\".join(parts) if parts else \"model_k2_p10_20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb8ee8-4327-49f7-a3c0-1f64322cc264",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# 14. Main Execution Block\n",
    "######################################\n",
    "def main_worker(rank: int, world_size: int, args):\n",
    "    # ‚Äî‚Äî‚Äî DDP setup ‚Äî‚Äî‚Äî\n",
    "    torch.cuda.set_device(rank)\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "    # 3) index files\n",
    "    main_folder = args.data_path\n",
    "    if not os.path.isdir(main_folder):\n",
    "        sys.exit(f\"ERROR: Data folder not found: {main_folder}\")\n",
    "    file_entries = []\n",
    "    print(f\"[Rank {rank}] indexing `{main_folder}` ‚Ä¶\")\n",
    "    for root, dirs, files in os.walk(main_folder):\n",
    "        for f in files:\n",
    "            if f.endswith(\".mat\"):\n",
    "                try:\n",
    "                    parent_path = os.path.dirname(os.path.join(root, f))\n",
    "                    parent = os.path.basename(parent_path)\n",
    "                    grandparent = os.path.basename(os.path.dirname(parent_path))\n",
    "                    if \"lt_\" in parent:\n",
    "                        raw_lt_str = parent.split(\"lt_\")[1]\n",
    "                    elif \"lt_\" in grandparent:\n",
    "                        raw_lt_str = grandparent.split(\"lt_\")[1]\n",
    "                    else:\n",
    "                        continue\n",
    "                    raw_lt = float(raw_lt_str)\n",
    "                    file_entries.append((os.path.join(root, f), depth_value, lifetime_value))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    if not file_entries:\n",
    "        sys.exit(\"No data files found. Exiting.\")\n",
    "    np.random.shuffle(file_entries)\n",
    "    train_entries, val_entries = train_test_split(\n",
    "        file_entries,\n",
    "        test_size=0.2,\n",
    "        random_state=args.global_seed,\n",
    "    )\n",
    "    if rank == 0:\n",
    "        print(f\"  Total files: {len(file_entries)}, train: {len(train_entries)}, val: {len(val_entries)}\")\n",
    "\n",
    "    import math\n",
    "    # ‚îÄ‚îÄ shard file list by rank ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "    def shard_list(lst, rank, world_size):\n",
    "        per = math.ceil(len(lst) / world_size)\n",
    "        start = rank * per\n",
    "        end   = min(start + per, len(lst))\n",
    "        return lst[start:end]\n",
    "\n",
    "    # 4) shard file list by process rank, then make loaders \n",
    "    train_entries_rank = train_entries[rank::world_size]\n",
    "    val_entries_rank   = val_entries[rank::world_size]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.global_batch_size // world_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.global_batch_size // world_size,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # 5) build model + wrap in DDP\n",
    "    device = torch.device(\"cuda\", rank)\n",
    "    config = EvidenceMoeConfig(\n",
    "        hidden_size=224,\n",
    "        intermediate_size=384,\n",
    "        num_hidden_layers=2,\n",
    "        num_attention_heads=16,\n",
    "        expert_lr=5e-5,\n",
    "        decider_lr=1e-4,\n",
    "        critic_lr=1e-4,\n",
    "        critic_weight_decay=1e-4,\n",
    "        lambda_KL=1e-3,\n",
    "        gamma_penalty=1e-3,\n",
    "        quality_kappa=2.0,\n",
    "        lambda_primary=1.0,\n",
    "        lambda_aux=1.0,\n",
    "        lambda_critic_quality=1.0,\n",
    "        lambda_corr=1.0,\n",
    "        lambda_penalty=1.0,\n",
    "        lambda_diversity=0.0,\n",
    "        damping_factor=0.1,\n",
    "       # Select Ablations for this run\n",
    "        ablate_quality_weighting=False, ablate_correction=False, ablate_quality_in_gating=False,\n",
    "        ablate_decider_feature=False, ablate_decider_feature_fusion=False, ablate_uniform_gating=False,\n",
    "        ablate_gating_dropout=False, ablate_phased_training = False, ablate_mean_pooling=False,\n",
    "        ablate_auxiliary_mae=False,\n",
    "    )\n",
    "    start_mode = \"full\" if config.ablate_phased_training else \"pretrain_experts\"\n",
    "    model = EvidenceMoeModel(config, mode=start_mode).to(device)\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "    model.config = config\n",
    "    # 6) train\n",
    "    best_val = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        num_epochs=500,\n",
    "        pretrain_epochs=10,\n",
    "        integration_epochs=20,\n",
    "    )\n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"\\n Training complete. Best Val MAE: {best_val:.6f}\")\n",
    "\n",
    "    # 7) cleanup\n",
    "    dist.barrier()\n",
    "    dist.destroy_process_group()\n",
    "def launch_ddp(world_size: int, args):\n",
    "    procs = []\n",
    "    for rank in range(world_size):\n",
    "        p = mp.Process(target=main_worker, args=(rank, world_size, args))\n",
    "        p.start()\n",
    "        procs.append(p)\n",
    "    for p in procs:\n",
    "        p.join()\n",
    "def pick_free_port():\n",
    "    \"\"\"Find an unused TCP port on localhost.\"\"\"\n",
    "    import socket\n",
    "    sock = socket.socket()\n",
    "    sock.bind((\"127.0.0.1\", 0))\n",
    "    _, port = sock.getsockname()\n",
    "    sock.close()\n",
    "    return port\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # manually build an args object\n",
    "    from types import SimpleNamespace\n",
    "    \n",
    "    args = SimpleNamespace(\n",
    "        data_path=\"\",\n",
    "        global_batch_size=512,\n",
    "        global_seed=42,\n",
    "        num_workers=8,\n",
    "    )\n",
    "    \n",
    "    # set up DDP env (you can keep your pick_free_port() code above)\n",
    "    port = pick_free_port()\n",
    "    os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "    os.environ[\"MASTER_PORT\"] = str(port)\n",
    "    \n",
    "    ngpus = torch.cuda.device_count()\n",
    "    assert ngpus > 1, \"Need multiple GPUs for DDP.\"\n",
    "    \n",
    "    # launch training\n",
    "    launch_ddp(ngpus, args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ae25c-fa32-4500-a676-a940d1d6e3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ba0bcb-436d-4513-ae04-1bda0ad8bb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e30b55-7d69-4787-ac8f-8b7c5ea20f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157dc68-b39b-4922-83f5-1ecd356269b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
